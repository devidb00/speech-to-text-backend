{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 242,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import re\n",
    "import string\n",
    "import requests\n",
    "import pandas as pd\n",
    "import qalsadi.lemmatizer\n",
    "from bs4 import BeautifulSoup\n",
    "import pyarabic.araby as araby\n",
    "from nltk.corpus import stopwords\n",
    "import arabicstopwords.arabicstopwords as stp\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 188,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_surah_names() -> list:\n",
    "    surah_names = [] #surah names sorted\n",
    "    URL = \"https://surahquran.com/quran-search/quran.html\"\n",
    "    page = requests.get(URL)\n",
    "    soup = BeautifulSoup(page.content, \"html.parser\")\n",
    "    all_table = soup.find_all('table')[1]\n",
    "    for idx, elm in enumerate(all_table.find_all(\"a\")):\n",
    "        surah_names.append(\n",
    "            (idx + 1, elm.text)\n",
    "        )\n",
    "    return surah_names"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 235,
   "metadata": {},
   "outputs": [],
   "source": [
    "LANGUAGE = \"AR\"\n",
    "file = 'arabic.csv' if LANGUAGE == \"AR\" else 'french.csv'\n",
    "\n",
    "book = pd.read_csv(\n",
    "    f'{os.getcwd()}/data/{file}',\n",
    "    sep=\"|\",\n",
    "    header=None,\n",
    ")\n",
    "\n",
    "book.columns = [\"surah\", \"ayat\", \"text\"]\n",
    "\n",
    "book['surah'] = book['surah'].astype(int)\n",
    "book['ayat'] = book['ayat'].astype(int)\n",
    "book['text'] = book['text'].apply(lambda t: araby.strip_diacritics(t))\n",
    "\n",
    "mapping_dictionary = {t[0]: t for t in get_surah_names()}\n",
    "book['surah_name'] = book['surah'].map(mapping_dictionary).apply(lambda t: t[1])\n",
    "\n",
    "book['text'].to_csv(f'{os.getcwd()}/output.txt', index=False, sep=\"\\t\", header=None)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<p>\n",
    "    Testing TheFuzz package\n",
    "</p>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 201,
   "metadata": {},
   "outputs": [],
   "source": [
    "from thefuzz import fuzz\n",
    "\n",
    "search_text = \"صراط الذين\"\n",
    "book = book.assign(\n",
    "    score = lambda _df: \n",
    "        _df['text'].apply(lambda t: \n",
    "            fuzz.partial_ratio(t, search_text)\n",
    "        )\n",
    "    )"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2>Text cleaning</h2>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 236,
   "metadata": {},
   "outputs": [],
   "source": [
    "def normalize_chars(txt) -> str:\n",
    "    txt = re.sub(\"[إأٱآا]\", \"ا\", txt)\n",
    "    txt = re.sub(\"ى\", \"ي\", txt)\n",
    "    txt = re.sub(\"ة\", \"ه\", txt)\n",
    "    return txt\n",
    "\n",
    "stopwordlist = set(list(stp.stopwords_list()) + stopwords.words('arabic'))\n",
    "stopwordlist = [normalize_chars(word) for word in stopwordlist]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 237,
   "metadata": {},
   "outputs": [],
   "source": [
    "lemmer = qalsadi.lemmatizer.Lemmatizer()\n",
    "\n",
    "def clean_txt(txt: str) -> str:\n",
    "    txt = araby.strip_diacritics(txt)\n",
    "    txt = araby.strip_tatweel(txt)\n",
    "    txt = normalize_chars(txt)\n",
    "    txt = ' '.join([\n",
    "        token.translate(\n",
    "            str.maketrans(\n",
    "                '', \n",
    "                '', \n",
    "                string.punctuation\n",
    "            )\n",
    "        ) for token in txt.split(' ') if token not in stopwordlist\n",
    "    ])\n",
    "    txt_lemmatized = ' '.join([lemmer.lemmatize(token) for token in txt.split(' ')])\n",
    "    return f\"{txt} {txt_lemmatized}\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 238,
   "metadata": {},
   "outputs": [],
   "source": [
    "book['clean_txt'] = book['text'].apply(lambda x: clean_txt(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 248,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array(['آتى', 'آتى أجزى', 'آتى اتوا', 'آتى اجرا', 'آتى اجرها',\n",
       "       'آتى اجرهم', 'آتى اجوركم', 'آتى اجورهم', 'آتى احدا', 'آتى الاخره',\n",
       "       'آتى الحكمه', 'آتى الزكاه', 'آتى الله', 'آتى اوتي', 'آتى اوتيتم',\n",
       "       'آتى باذن', 'آتى تب', 'آتى حذر', 'آتى خير', 'آتى ردى'],\n",
       "      dtype=object)"
      ]
     },
     "execution_count": 248,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "corpus = book[\"clean_txt\"]\n",
    "vectorizer = TfidfVectorizer(ngram_range=(1, 2))\n",
    "corpus_vectorized = vectorizer.fit_transform(corpus)\n",
    "\n",
    "vectorizer.get_feature_names_out()[:20]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 282,
   "metadata": {},
   "outputs": [],
   "source": [
    "def show_best_results(\n",
    "    df: pd.DataFrame, \n",
    "    scores_array: list, \n",
    "    top_n: int = 2\n",
    ") -> None:\n",
    "    sorted_indices = scores_array.argsort()[::-1]\n",
    "    for idx in sorted_indices[:top_n]:\n",
    "        row = df.iloc[idx]\n",
    "        surah = row[\"surah\"]\n",
    "        text = row[\"text\"]\n",
    "        ayah_num = row[\"ayat\"]\n",
    "        surah_name = row[\"surah_name\"]\n",
    "        score = scores_array[idx]\n",
    "        if score > 0:\n",
    "            print(f\"Surat nb: {surah}  | Ayat: {ayah_num} | Surah: {surah_name} | Score: {score}\")\n",
    "            print(text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 283,
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_tfidf(query: str) -> None:\n",
    "    query = clean_txt(query)\n",
    "    query_vectorized = vectorizer.transform([query])\n",
    "    scores = query_vectorized.dot(corpus_vectorized.transpose())\n",
    "    scores_array = scores.toarray()[0]\n",
    "    show_best_results(book, scores_array)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 286,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Surat nb: 9  | Ayat: 117 | Surah: التوبة | Score: 0.6379697081810952\n",
      "لقد تاب الله على النبي والمهاجرين والأنصار الذين اتبعوه في ساعة العسرة من بعد ما كاد يزيغ قلوب فريق منهم ثم تاب عليهم  إنه بهم رءوف رحيم\n",
      "Surat nb: 3  | Ayat: 8 | Surah: آل عمران | Score: 0.11008765329491813\n",
      "ربنا لا تزغ قلوبنا بعد إذ هديتنا وهب لنا من لدنك رحمة  إنك أنت الوهاب\n"
     ]
    }
   ],
   "source": [
    "q = \"أين أول؟ لقد تنبى الله علم النبي والمهاجرين، والأورو صار الذين تبعوه الذين اتبعوه في ساعة العسرة بعد ما كاد يزيغ قلوب فريق منهم، ثم.\" \n",
    "run_tfidf(q)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
